{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d21354",
   "metadata": {},
   "source": [
    "# API ELT Notebook\n",
    "\n",
    "This notebook demonstrates an Extract, Load, and Transform (ELT) process using data from an API. The following sections provide a step-by-step implementation of the pipeline, including API interaction, data transformation, and database integration.\n",
    "\n",
    "## Sections Overview\n",
    "\n",
    "1. **API Interaction**: Fetch data from the API using Python libraries.\n",
    "2. **Data Transformation**: Process and clean the data for analysis.\n",
    "3. **Database Integration**: Store the transformed data into a database for further use.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e75db2",
   "metadata": {},
   "source": [
    "\n",
    "### SECTION 1: IMPORTS AND ENVIRONMENT SETUP\n",
    "> This section imports all required libraries and loads environment variables\n",
    "> for API authentication and database connectivity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1825d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydotenv import Environment\n",
    "import datetime\n",
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import mapped_column, Mapped, declarative_base\n",
    "\n",
    "# Load environment variables from local.env file\n",
    "# Contains sensitive credentials like API keys and database connection details\n",
    "env = Environment(\"./local.env\")\n",
    "API_KEY = env.get(\"weather_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a5fb63",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. CONFIGURATION AND DATABASE SETUP\n",
    "> Define API endpoint template and establish database connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# API endpoint template with placeholders for dynamic parameters\n",
    "# Used to construct URLs for weather API requests\n",
    "base_url = \"http://api.weatherapi.com/v1/{method}.json?key={api_key}&q={location}&dt={date}\"\n",
    "\n",
    "# Create database engine for PostgreSQL connection\n",
    "# Uses credentials from environment variables for security\n",
    "engine = create_engine(\n",
    "    f'postgresql://{env.get(\"db_login\")}@{env.get(\"db_host\")}/{env.get(\"db_name\")}', \n",
    "    echo=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effabae",
   "metadata": {},
   "source": [
    "## 3. Extract: Fetch Historical Weather Data from API\n",
    "\n",
    "This section retrieves historical weather data for Lagos from the Weather API.\n",
    "\n",
    "**Process Overview:**\n",
    "- Iterates through date range (last 90 days to today)\n",
    "- Constructs API URLs with proper parameters\n",
    "- Makes HTTP requests with error handling\n",
    "- Implements rate limiting with 1-second delays\n",
    "- Stores all API responses for downstream processing\n",
    "\n",
    "**Key Features:**\n",
    "- Date formatting and incrementation\n",
    "- Status code validation\n",
    "- Exception handling for API errors\n",
    "- Progress reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729cdde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize date range: start from 1 day ago, end at today\n",
    "current_datetime = datetime.datetime.today() - datetime.timedelta(days=14)  # Start date: 14 days ago\n",
    "today = datetime.datetime.today()  # End date: today\n",
    "responses = []  # List to store all API responses for processing\n",
    "\n",
    "# Main extraction loop: iterates through each day in the date range\n",
    "while True: \n",
    "    try:\n",
    "        # Format current date as 'YYYY-MM-DD' for API query\n",
    "        current_date = current_datetime.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Construct the API URL with formatted parameters\n",
    "        # method: 'history' for historical data\n",
    "        # location: 'Lagos' (hardcoded, can be parameterized)\n",
    "        # date: current iteration date\n",
    "        api_url = base_url.format(\n",
    "            method='history', \n",
    "            api_key=API_KEY,\n",
    "            location='Lagos',\n",
    "            date=current_date\n",
    "        )\n",
    "\n",
    "        # Log progress for monitoring\n",
    "        print(\"Fetching records for date: \", current_date, end='...')\n",
    "        \n",
    "        # Make HTTP GET request to the API\n",
    "        response = requests.get(api_url)\n",
    "        \n",
    "        # Validate response status code\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Error: {response.status_code}\")\n",
    "        \n",
    "        # Confirm successful request and store response\n",
    "        print('Success.')\n",
    "        responses.append(response.json())\n",
    "        \n",
    "        # Increment date by 1 day for next iteration\n",
    "        current_datetime = current_datetime + datetime.timedelta(days=1)\n",
    "        \n",
    "        # Check if we've reached today's date; if so, exit loop\n",
    "        if (current_datetime - today).days == 0:\n",
    "            print(\"All records fetched successfully\")\n",
    "            break\n",
    "        \n",
    "        # Introduce 1-second delay to respect API rate limits\n",
    "        sleep(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa71303",
   "metadata": {},
   "source": [
    "## 4. Transform: Parse and Structure API Response Data\n",
    "\n",
    "This section transforms the nested JSON responses from the API into structured pandas DataFrames.\n",
    "\n",
    "**Data Extraction Strategy:**\n",
    "- **Hourly Data**: Extracts detailed hourly weather metrics from nested forecast arrays\n",
    "- **Astronomical Data**: Extracts sunrise, sunset, and moonphase information\n",
    "- **Daily Data**: Extracts daily aggregated weather metrics and conditions\n",
    "\n",
    "**Transformation Steps:**\n",
    "1. Flatten nested JSON structures using `json_normalize()`\n",
    "2. Extract specific forecast data for each day\n",
    "3. Combine related data (e.g., condition details) into single records\n",
    "4. Validate dimensions of resulting DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efefbd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract hourly weather data from all responses\n",
    "# json_normalize flattens the nested structure (response > forecast > forecastday > hour)\n",
    "hour_dfs = [\n",
    "    pd.json_normalize(response, record_path=['forecast', 'forecastday', 'hour']) \n",
    "    for response in responses\n",
    "]\n",
    "# Concatenate all hourly dataframes into a single dataframe\n",
    "hour_df = pd.concat(hour_dfs, ignore_index=True, axis=0)\n",
    "\n",
    "# Extract astronomical data (sunrise, sunset, moonphase)\n",
    "# Takes the first (and only) forecastday's astro data from each response\n",
    "astro_df = pd.DataFrame([\n",
    "    response['forecast']['forecastday'][0]['astro'] \n",
    "    for response in responses\n",
    "])\n",
    "\n",
    "# Extract daily weather data (temperature, precipitation, conditions)\n",
    "day_df = pd.DataFrame([\n",
    "    response['forecast']['forecastday'][0]['day'] \n",
    "    for response in responses\n",
    "])\n",
    "\n",
    "# Flatten condition nested object into separate columns\n",
    "# Drop the original 'condition' column and add its normalized components\n",
    "day_df = pd.concat(\n",
    "    [\n",
    "        day_df.drop('condition', axis=1),  # All columns except 'condition'\n",
    "        pd.json_normalize(day_df.condition)  # Normalized 'condition' data\n",
    "    ], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display dataframe dimensions for verification\n",
    "print(\"Hourly data shape:\", hour_df.shape)\n",
    "print(\"Astronomical data shape:\", astro_df.shape)\n",
    "print(\"Daily data shape:\", day_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058e7bc",
   "metadata": {},
   "source": [
    "## 5. Load: Persist Transformed Data to PostgreSQL Database\n",
    "\n",
    "This final section loads the three transformed dataframes into dedicated PostgreSQL tables.\n",
    "\n",
    "**Tables Created/Updated:**\n",
    "- `hourly_weather`: Detailed hourly weather observations\n",
    "- `astro_weather`: Astronomical phenomena data (sunrise, sunset, moon phase)\n",
    "- `daily_weather`: Daily aggregated weather summaries\n",
    "\n",
    "**Configuration:**\n",
    "- `if_exists='append'`: Adds new records to existing tables without truncating\n",
    "- `index=False`: Excludes pandas index from being written to database\n",
    "- Connection uses SQLAlchemy engine configured in Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load hourly weather data\n",
    "# Table: hourly_weather\n",
    "# Records: One row per hour per day in the date range\n",
    "hour_df.to_sql('hourly_weather', con=engine, if_exists='append', index=False)\n",
    "print(\"✓ Hourly weather data loaded successfully\")\n",
    "\n",
    "# Load astronomical data\n",
    "# Table: astro_weather\n",
    "# Records: One row per day (sunrise, sunset, moonphase information)\n",
    "astro_df.to_sql('astro_weather', con=engine, if_exists='append', index=False)\n",
    "print(\"✓ Astronomical weather data loaded successfully\")\n",
    "\n",
    "# Load daily weather summary data\n",
    "# Table: daily_weather\n",
    "# Records: One row per day with aggregated metrics (min/max temps, precipitation, etc.)\n",
    "day_df.to_sql('daily_weather', con=engine, if_exists='append', index=False)\n",
    "print(\"✓ Daily weather data loaded successfully\")\n",
    "\n",
    "print(\"\\nELT Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a9bfd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This ELT pipeline successfully:\n",
    "1. **Extracted** historical weather data from the Weather API (90-day lookback)\n",
    "2. **Transformed** nested JSON responses into three structured dataframes\n",
    "3. **Loaded** the data into PostgreSQL tables for analysis\n",
    "\n",
    "**Next Steps:**\n",
    "- Verify data integrity in database tables\n",
    "- Build analytical queries on weather patterns\n",
    "- Create visualizations for weather trends\n",
    "- Set up scheduled runs using Apache Airflow or similar orchestration tool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
